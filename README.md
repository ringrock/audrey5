# ğŸ¤– AskMe - Assistant AI Multi-Client

[![GitHub release](https://img.shields.io/github/v/release/avanteam/askme-app-aoai)](https://github.com/avanteam/askme-app-aoai/releases)
[![Docker](https://img.shields.io/badge/docker-Harbor%20OVH-blue)](https://7wpjr0wh.c1.gra9.container-registry.ovh.net)
[![Kubernetes](https://img.shields.io/badge/kubernetes-1.21+-green)](https://kubernetes.io)
[![Tests](https://img.shields.io/badge/tests-passing-brightgreen)](./tests/)

AskMe est un assistant virtuel d'entreprise multi-client qui supporte plusieurs fournisseurs LLM et se dÃ©ploie facilement via Kubernetes/Rancher.

## ğŸ¯ FonctionnalitÃ©s

### ğŸ§  Multi-LLM Support
- **Azure OpenAI** - Service Azure avec intÃ©gration native
- **Claude** - Anthropic Claude 4 Sonnet pour des rÃ©ponses prÃ©cises
- **OpenAI Direct** - API OpenAI directe (GPT-4o)
- **Mistral** - ModÃ¨les Mistral AI open-source
- **Gemini** - Google Gemini pour la diversitÃ© des rÃ©ponses

### ğŸ¢ Architecture Multi-Client
- **Isolation complÃ¨te** par namespace Kubernetes
- **Configuration personnalisÃ©e** par client via Rancher UI
- **DNS automatique** avec gestion OVH intÃ©grÃ©e
- **Scaling indÃ©pendant** par dÃ©ploiement

### ğŸ¤ FonctionnalitÃ©s AvancÃ©es
- **Reconnaissance vocale** avec mots-clÃ©s d'activation
- **SynthÃ¨se vocale** Azure Speech Services
- **Upload d'images** avec analyse multimodale
- **Historique conversations** stockÃ© en CosmosDB
- **Citations automatiques** depuis Azure Search

## ğŸš€ DÃ©marrage Rapide

### PrÃ©requis
- **Docker** & **Docker Compose**
- **Node.js 20+** pour le dÃ©veloppement frontend
- **Python 3.11+** pour le backend
- **Kubernetes** cluster pour la production

### Installation Locale

```bash
# 1. Cloner le repository
git clone https://github.com/avanteam/askme-app-aoai.git
cd askme-app-aoai

# 2. Configuration
cp .env.sample .env
# Ã‰diter .env avec vos clÃ©s API

# 3. DÃ©marrage (build frontend + backend)
./start.sh
```

L'application sera disponible sur http://localhost:50505

### DÃ©ploiement Production

Pour dÃ©ployer en production via Rancher :

```bash
# 1. CrÃ©er une version
./scripts/release-sync.sh

# 2. DÃ©ployer via Rancher UI ou CLI
./scripts/deploy-client.sh client-name domain.com
```

## âš™ï¸ Configuration

### Variables d'Environnement Principales

```env
# Provider LLM par dÃ©faut
LLM_PROVIDER=CLAUDE
AVAILABLE_LLM_PROVIDERS=AZURE_OPENAI,CLAUDE,OPENAI_DIRECT,MISTRAL,GEMINI

# Azure OpenAI
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
AZURE_OPENAI_KEY=your_azure_key
AZURE_OPENAI_MODEL=gpt-4o

# Claude AI
CLAUDE_API_KEY=your_claude_key
CLAUDE_MODEL=claude-sonnet-4-20250514

# OpenAI Direct
OPENAI_DIRECT_API_KEY=your_openai_key
OPENAI_DIRECT_MODEL=gpt-4o

# Azure Search (donnÃ©es contextuelles)
AZURE_SEARCH_SERVICE=your-search-service
AZURE_SEARCH_INDEX=your-index
AZURE_SEARCH_KEY=your_search_key
```

Voir `.env.sample` pour la configuration complÃ¨te.

### Interface de Personnalisation

L'application propose une interface graphique permettant aux utilisateurs de :
- **Changer de provider LLM** en temps rÃ©el
- **Ajuster la longueur des rÃ©ponses** (courtes, normales, dÃ©taillÃ©es)
- **Modifier le nombre de documents** de rÃ©fÃ©rence
- **Configurer la reconnaissance vocale**

## ğŸ—ï¸ Architecture

### Backend (Python/Quart)
```
backend/
â”œâ”€â”€ llm_providers/          # Abstraction multi-LLM
â”‚   â”œâ”€â”€ azure_openai.py     # Provider Azure OpenAI
â”‚   â”œâ”€â”€ claude.py           # Provider Anthropic Claude
â”‚   â”œâ”€â”€ openai_direct.py    # Provider OpenAI Direct
â”‚   â””â”€â”€ ...
â”œâ”€â”€ auth/                   # Authentification
â”œâ”€â”€ history/                # Gestion historique
â””â”€â”€ settings.py             # Configuration centralisÃ©e
```

### Frontend (React/TypeScript)
```
frontend/src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ Answer/             # Affichage des rÃ©ponses
â”‚   â”œâ”€â”€ QuestionInput/      # Interface de saisie
â”‚   â””â”€â”€ Customization/      # Panneau de personnalisation
â”œâ”€â”€ hooks/
â”‚   â””â”€â”€ useVoiceRecognition.ts
â””â”€â”€ state/                  # Gestion d'Ã©tat globale
```

### Infrastructure
```
helm-chart/                 # DÃ©ploiement Kubernetes
â”œâ”€â”€ templates/              # Manifestes K8s
â”œâ”€â”€ values.yaml            # Configuration par dÃ©faut
â””â”€â”€ Chart.yaml             # MÃ©tadonnÃ©es Helm
```

## ğŸ§ª Tests

### ExÃ©cution des Tests

```bash
# Tests complets
npm test                    # Frontend
pytest                     # Backend
./scripts/test-workflow.sh  # Tests d'intÃ©gration

# Tests par catÃ©gorie
pytest tests/unit_tests/           # Tests unitaires
pytest tests/functional_tests/    # Tests fonctionnels
pytest tests/integration_tests/   # Tests d'intÃ©gration
```

### Tests E2E

```bash
# Tests End-to-End avec Playwright
cd tests/e2e
npm install
npx playwright test
```

## ğŸ”§ DÃ©veloppement

### Architecture des Providers LLM

Chaque provider implÃ©mente l'interface `LLMProvider` :

```python
class LLMProvider(ABC):
    @abstractmethod
    async def send_request(self, messages: List[Dict], stream: bool = True, **kwargs):
        """Envoyer une requÃªte au provider"""
        pass
    
    @abstractmethod  
    def format_response(self, raw_response: Any, stream: bool = True):
        """Formater la rÃ©ponse en format standard"""
        pass
```

### Ajout d'un Nouveau Provider

1. CrÃ©er `backend/llm_providers/nouveau_provider.py`
2. ImplÃ©menter la classe `NouveauProvider(LLMProvider)`
3. Ajouter dans `backend/llm_providers/__init__.py`
4. Ajouter la configuration dans `backend/settings.py`

### Commandes de DÃ©veloppement

```bash
# Frontend
cd frontend
npm run dev                 # Serveur de dÃ©veloppement
npm run build              # Build production
npm run lint               # VÃ©rification code

# Backend  
python -m uvicorn app:app --port 50505 --reload
python -m pytest --cov    # Tests avec couverture
```

## ğŸ“Š Monitoring

### Surveillance des DÃ©ploiements

```bash
# Dashboard en temps rÃ©el
./scripts/monitor-clients.sh

# Status d'un client spÃ©cifique
kubectl get pods -n askme-client-name
kubectl logs deployment/askme-app -n askme-client-name
```

### MÃ©triques Disponibles

- **Performance** : Temps de rÃ©ponse LLM par provider
- **Utilisation** : Nombre de conversations par client
- **Ressources** : CPU, RAM, stockage par dÃ©ploiement
- **SantÃ©** : Status des pods et services

## ğŸ”„ Workflow de Release

### 1. DÃ©veloppement
```bash
git checkout test-rg2
# ... dÃ©veloppement ...
git commit -m "feat: nouvelle fonctionnalitÃ©"
git push origin test-rg2
```

### 2. Release
```bash
# Synchronisation des versions entre repositories
./scripts/release-sync.sh
# Choisir version (ex: v1.2.0)
# Tags automatiquement les deux repos
```

### 3. DÃ©ploiement
- **Automatique** : Pipeline CI/CD build l'image Docker et package Helm
- **Manuel** : Interface Rancher ou script CLI

### 4. Mise Ã  Jour Client
- SÃ©lection de version dans Rancher UI
- Rolling update sans interruption
- Rollback 1-clic en cas de problÃ¨me

## ğŸ¤ Contribution

### Standards de Code

- **Python** : PEP 8, type hints obligatoires
- **TypeScript** : Airbnb config, composants fonctionnels
- **Git** : Conventional commits, rebase workflow
- **Documentation** : Inline + README mis Ã  jour

### Pull Request

1. Fork du repository
2. Feature branch depuis `test-rg2`
3. Tests passants requis
4. Documentation mise Ã  jour
5. Review avant merge

## ğŸ“„ Licence

Copyright Â© 2025 Avanteam. Tous droits rÃ©servÃ©s.

## ğŸ†˜ Support

- **Documentation** : Voir `/docs` et guides spÃ©cialisÃ©s
- **Issues** : [GitHub Issues](https://github.com/avanteam/askme-app-aoai/issues)
- **Support** : Ã‰quipe DevOps Avanteam

---

<div align="center">

**[Rancher Catalog](https://github.com/avanteam/askme-rancher-catalog)** â€¢ **[Documentation ComplÃ¨te](./CLAUDE.md)** â€¢ **[Guide Workflow](./WORKFLOW_RELEASE.md)**

</div>