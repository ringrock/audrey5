# Tests Fonctionnels AskMe - Multi-LLM

Ce rÃ©pertoire contient une suite complÃ¨te de tests fonctionnels pour l'application AskMe avec support de tous les LLM intÃ©grÃ©s.

## ğŸ¯ Vue d'ensemble

Les tests vÃ©rifient le bon fonctionnement de l'application avec tous les LLM supportÃ©s :
- **AZURE_OPENAI** - Azure OpenAI Service
- **CLAUDE** - Anthropic Claude AI  
- **OPENAI_DIRECT** - API OpenAI directe
- **MISTRAL** - Mistral AI
- **GEMINI** - Google Gemini AI

## ğŸ“ Structure des Tests

```
tests/
â”œâ”€â”€ functional_tests/           # Tests fonctionnels principaux
â”‚   â”œâ”€â”€ test_language.py       # Tests de langue (franÃ§ais/anglais/italien)
â”‚   â”œâ”€â”€ test_search.py         # Tests Azure AI Search avec citations
â”‚   â”œâ”€â”€ test_response_length.py # Tests longueur rÃ©ponse (short/medium/long)
â”‚   â”œâ”€â”€ test_document_count.py # Tests nombre de documents (2/6/12)
â”‚   â””â”€â”€ conftest.py            # Configuration pytest pour tests fonctionnels
â”œâ”€â”€ integration_tests/         # Tests d'intÃ©gration existants
â”œâ”€â”€ unit_tests/               # Tests unitaires existants
â”œâ”€â”€ run_test.py              # Runner principal de tests
â”œâ”€â”€ pytest.ini              # Configuration pytest
â””â”€â”€ README.md               # Cette documentation
```

## ğŸš€ Utilisation Rapide

### Installation et Configuration

```bash
# CrÃ©er et configurer l'environnement de test
python tests/run_test.py --setup-only

# Lister les tests disponibles
python tests/run_test.py --list
```

### ExÃ©cution des Tests

```bash
# Tous les tests fonctionnels pour tous les LLM
python tests/run_test.py --type functional

# Tests pour un LLM spÃ©cifique
python tests/run_test.py --type functional --llm AZURE_OPENAI

# Tests avec exclusion d'un LLM
python tests/run_test.py --type functional --llm-skip GEMINI

# Tests par catÃ©gorie (markers)
python tests/run_test.py --type functional --markers language
python tests/run_test.py --type functional --markers search,response_length

# Tests d'images seulement (LLM supportÃ©s)
python tests/run_test.py --type functional --markers image --llm CLAUDE,GEMINI,OPENAI_DIRECT

# Mode verbose avec rapport HTML
python tests/run_test.py --type functional --verbose --html-report
```

## ğŸ§ª Types de Tests Disponibles

### 1. Tests de Langue (`test_language.py`)

VÃ©rification du support multilingue :

| Test | Description | Validation |
|------|-------------|------------|
| **test_french_identity_question** | Question "Qui es-tu ?" en franÃ§ais | RÃ©ponse mentionne "AskMe" ou Ã©quivalent |
| **test_english_language_response** | Question "Who are you?" en anglais | RÃ©ponse en anglais |
| **test_italian_poem_generation** | Demande de poÃ¨me en italien | PoÃ¨me gÃ©nÃ©rÃ© en italien |
| **test_language_consistency** | CohÃ©rence entre langues | RÃ©ponses diffÃ©rentes selon la langue |

### 2. Tests Azure AI Search (`test_search.py`)

VÃ©rification de l'intÃ©gration avec Azure AI Search :

| Test | Description | Validation |
|------|-------------|------------|
| **test_search_french_with_citations** | Recherche nouveautÃ©s en franÃ§ais | Citations Azure AI Search prÃ©sentes |
| **test_search_spanish_with_citations** | MÃªme recherche en espagnol | Citations prÃ©sentes, rÃ©ponse en espagnol |
| **test_search_consistency_between_languages** | CohÃ©rence citations | Documents similaires entre langues |
| **test_search_without_results** | Question sans rÃ©sultats pertinents | Gestion gracieuse sans erreur |

### 3. Tests Longueur de RÃ©ponse (`test_response_length.py`)

VÃ©rification du respect des contraintes de longueur :

| Test | Description | Validation |
|------|-------------|------------|
| **test_response_length_progression** | Progression short â†’ medium â†’ long | `short < medium < long` en nombre de mots |
| **test_short_response_quality** | QualitÃ© rÃ©ponses courtes | 10-150 mots, informative |
| **test_long_response_depth** | Profondeur rÃ©ponses longues | â‰¥100 mots, structure dÃ©taillÃ©e |
| **test_response_length_with_search** | Longueurs avec recherche | Progression respectÃ©e avec citations |

### 4. Tests Nombre de Documents (`test_document_count.py`)

VÃ©rification du paramÃ¨tre `documents_count` :

| Test | Description | Validation |
|------|-------------|------------|
| **test_document_count_progression** | Progression 2 â†’ 6 â†’ 12 documents | Plus de documents = plus de contenu |
| **test_limited_documents_available** | Peu de documents disponibles | Fonctionnement sans erreur |
| **test_document_count_consistency** | CohÃ©rence entre nombres | Subset des citations |
| **test_document_count_quality** | QualitÃ© avec beaucoup de docs | Citations valides, pas de rÃ©pÃ©titions |

### 5. Tests d'Images (`test_image_upload.py`)

VÃ©rification de l'analyse d'images pour **CLAUDE, GEMINI, OPENAI_DIRECT** :

| Test | Description | Validation |
|------|-------------|------------|
| **test_broken_bottle_analysis_french** | Image bouteille cassÃ©e + "Qu'est-ce que tu vois ?" | Identifie problÃ¨me production |
| **test_engine_fire_procedure_french** | Image moteur en feu + "Que faire ?" | ProcÃ©dures urgence + citations Azure AI Search |
| **test_broken_bottle_analysis_english** | MÃªme image + "What do you see?" | RÃ©ponse en anglais cohÃ©rente |
| **test_engine_fire_procedure_english** | MÃªme image + "What to do?" | ProcÃ©dures en anglais + citations |
| **test_image_analysis_consistency** | CohÃ©rence entre franÃ§ais/anglais | Contenu similaire, langues diffÃ©rentes |

## ğŸ“Š Markers et CatÃ©gories

Les tests utilisent des markers pytest pour une exÃ©cution sÃ©lective :

```bash
# Tests par marker
python tests/run_test.py --type functional --markers language    # Tests de langue
python tests/run_test.py --type functional --markers search      # Tests de recherche
python tests/run_test.py --type functional --markers response_length  # Tests longueur
python tests/run_test.py --type functional --markers document_count   # Tests documents
python tests/run_test.py --type functional --markers image       # Tests d'images
python tests/run_test.py --type functional --markers slow        # Tests lents

# Combinaisons de markers
python tests/run_test.py --type functional --markers "language or search"
python tests/run_test.py --type functional --markers "image and not slow"
python tests/run_test.py --type functional --markers "not slow"
```

## ğŸ›ï¸ Configuration et ParamÃ¨tres

### Variables d'Environnement

Les tests utilisent la configuration existante de l'application via les variables d'environnement du fichier `.env` :

```env
# LLM Provider Configuration
LLM_PROVIDER=AZURE_OPENAI
AVAILABLE_LLM_PROVIDERS=AZURE_OPENAI,CLAUDE,OPENAI_DIRECT,MISTRAL,GEMINI

# Azure OpenAI
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
AZURE_OPENAI_KEY=your_key
AZURE_OPENAI_MODEL=gpt-4

# Claude
CLAUDE_API_KEY=your_claude_key
CLAUDE_MODEL=claude-3-opus-20240229

# OpenAI Direct
OPENAI_DIRECT_API_KEY=your_openai_key
OPENAI_DIRECT_MODEL=gpt-4

# Mistral
MISTRAL_API_KEY=your_mistral_key
MISTRAL_MODEL=mistral-large-latest

# Gemini
GEMINI_API_KEY=your_gemini_key
GEMINI_MODEL=gemini-pro
```

### Fixtures Pytest

Les tests utilisent des fixtures paramÃ©trÃ©es pour tester tous les LLM automatiquement :

- `llm_provider_type` : ParamÃ¨tre qui itÃ¨re sur tous les LLM supportÃ©s
- `llm_provider` : Instance du provider LLM pour le test
- `test_messages_*` : Messages de test prÃ©dÃ©finis

## ğŸ“ˆ Rapports et RÃ©sultats

### Rapport HTML

```bash
python tests/run_test.py --type functional --html-report
# GÃ©nÃ¨re: tests/reports/report.html
```

### Couverture de Code

```bash
python tests/run_test.py --type functional --coverage
# GÃ©nÃ¨re: htmlcov/index.html
```

### Logs DÃ©taillÃ©s

```bash
python tests/run_test.py --type functional --verbose
```

## ğŸ”§ DÃ©pannage

### ProblÃ¨mes Courants

1. **Provider non disponible**
   ```
   pytest.skip: Provider CLAUDE non disponible: Missing API key
   ```
   â†’ VÃ©rifier les variables d'environnement pour le LLM

2. **Timeout sur tests lents**
   ```bash
   # Augmenter le timeout (dÃ©faut: 300s)
   python tests/run_test.py --type functional --markers "not slow"
   ```

3. **Environnement virtuel**
   ```bash
   # RecrÃ©er l'environnement de test
   rm -rf test_env
   python tests/run_test.py --setup-only
   ```

### Debug Mode

```bash
# Mode debug avec arrÃªt sur premiÃ¨re erreur
python tests/run_test.py --type functional --verbose --exit-on-fail -x
```

## ğŸ¯ Validation des Exigences

Les tests couvrent **toutes** les exigences spÃ©cifiÃ©es :

âœ… **Test 1** : Question "Qui es-tu ?" â†’ RÃ©ponse "AskMe"  
âœ… **Test 2** : Question anglaise â†’ RÃ©ponse en anglais  
âœ… **Test 3** : PoÃ¨me en italien â†’ PoÃ¨me gÃ©nÃ©rÃ© en italien  
âœ… **Test 4** : Recherche nouveautÃ©s â†’ Citations Azure AI Search  
âœ… **Test 5** : MÃªme recherche en espagnol â†’ Citations en espagnol  
âœ… **Test 6** : Longueurs short/medium/long â†’ Progression cohÃ©rente  
âœ… **Test 7** : Documents 2/6/12 â†’ Nombre de citations cohÃ©rent  
âœ… **Test 8** : Image bouteille cassÃ©e â†’ Analyse problÃ¨me production (FR/EN)  
âœ… **Test 9** : Image moteur en feu â†’ ProcÃ©dures urgence + citations (FR/EN)

**Pour TOUS les LLM supportÃ©s** : AZURE_OPENAI, CLAUDE, OPENAI_DIRECT, MISTRAL, GEMINI  
**Tests d'images** : CLAUDE, GEMINI, OPENAI_DIRECT

## ğŸ“ Maintenance

### Ajouter un Nouveau LLM

1. Ajouter le provider dans `SUPPORTED_LLMS` dans `run_test.py`
2. Les tests existants testeront automatiquement le nouveau LLM
3. Ajouter la configuration dans `.env`

### Ajouter un Nouveau Test

1. CrÃ©er le test dans le fichier appropriÃ© (`test_*.py`)
2. Utiliser les fixtures existantes (`llm_provider`, `llm_provider_type`)
3. Ajouter un marker si nÃ©cessaire dans `pytest.ini`

### Optimisation des Performances

```bash
# Tests en parallÃ¨le (si pytest-xdist installÃ©)
python tests/run_test.py --type functional -n auto

# Tests seulement pour LLM spÃ©cifiques
python tests/run_test.py --type functional --llm AZURE_OPENAI,CLAUDE
```